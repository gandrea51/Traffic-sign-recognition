import tensorflow as tf
from tensorflow.keras import models, layers
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def load_data(data_dir, classes):
    '''Function per il caricamento dei dati'''

    X_train = np.load(f'{data_dir}/X_augmented.npy')
    y_train = np.load(f'{data_dir}/y_augmented.npy')
    X_val = np.load(f'{data_dir}/X_val.npy')
    y_val = np.load(f'{data_dir}/y_val.npy')
    X_test = np.load(f'{data_dir}/X_test.npy')
    y_test = np.load(f'{data_dir}/y_test.npy')

    y_train = tf.keras.utils.to_categorical(y_train, classes)
    y_val = tf.keras.utils.to_categorical(y_val, classes)
    y_test = tf.keras.utils.to_categorical(y_test, classes)

    return X_train, y_train, X_val, y_val, X_test, y_test

#def lenet5(input_shape, classes):
    '''LeNet 5'''

    model = models.Sequential([
        layers.Input(shape=input_shape),
        layers.Conv2D(6, kernel_size=(5, 5), activation='relu', padding='same'),
        layers.AveragePooling2D(pool_size=(2, 2), strides=2),
        layers.Conv2D(16, kernel_size=(5, 5), strides=1, activation='relu'),
        layers.AveragePooling2D(pool_size=(2, 2), strides=2),
        layers.Flatten(),
        layers.Dense(units=120, activation='relu'),
        layers.Dense(units=84, activation='relu'),
        layers.Dense(units=classes, activation='softmax')
    ])
    return model

#def lenet5plus(input_shape, classes):
    '''LeNet 5 PLUS'''

    model = models.Sequential([
        layers.Input(shape=input_shape),
        
        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(pool_size=(2, 2), strides=2),
        
        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(pool_size=(2, 2), strides=2),
        
        layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D(pool_size=(2, 2), strides=2),
        
        layers.Flatten(),

        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(classes, activation='softmax')
    ])
    return model

def lenet5pro(input_shape, classes):
    '''LeNet 5 PRO'''

    model = models.Sequential([
        layers.Input(shape=input_shape),
        
        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.AveragePooling2D(pool_size=(2, 2), strides=2),
        
        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.AveragePooling2D(pool_size=(2, 2), strides=2),
        
        layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.AveragePooling2D(pool_size=(2, 2), strides=2),
        
        layers.Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.AveragePooling2D(pool_size=(2, 2), strides=2),

        layers.Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.AveragePooling2D(pool_size=(2, 2), strides=2),

        layers.GlobalAveragePooling2D(),
        
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(128, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(classes, activation='softmax')
    ])
    return model

# layers.GlobalAveragePooling2D() al posto di layers.Flatten() per una variante

def compile_net(model):
    '''Compile'''

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[tf.keras.metrics.Recall()])
    return model

def train_net(model, X_train, y_train, X_val, y_val, epochs=20, batch_size=128):
    '''Training'''

    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), verbose=1)
    return history

def evaluate_net(model, X_test, y_test):
    '''Evaluate'''

    score = model.evaluate(X_test, y_test, verbose=0)
    print('\nTest loss:', score[0])
    print('Test accuracy:', score[1])
    #print('Test recall:', score[1])
    return score

def plot_history(history):
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train acc')
    plt.plot(history.history['val_accuracy'], label='Validation acc')
    #plt.plot(history.history['recall'], label='Train recall')
    #plt.plot(history.history['val_recall'], label='Validation recall')
    plt.title('Accuracy')
    #plt.title('Recall')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train')
    plt.plot(history.history['val_loss'], label='Val')
    plt.title('Loss')
    plt.legend()
    plt.show()


data_dir = './dataset'
classes = 43
input_shape = (32, 32, 3)

X_train, y_train, X_val, y_val, X_test, y_test = load_data(data_dir, classes)

#model = lenet5(input_shape, classes)
#model = lenet5plus(input_shape, classes)
model = lenet5pro(input_shape, classes)

model = compile_net(model)
model.summary()

history = train_net(model, X_train, y_train, X_val, y_val)
evaluate_net(model, X_test, y_test)
plot_history(history)


y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

confusion = confusion_matrix(y_true_classes, y_pred_classes)

plt.figure(figsize=(10, 8))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_true_classes, y_pred_classes))

#model.save('./models/lenet.h5')
#model.save('./models/lenetplus.h5')
model.save('./models/lenetpro.h5')